---
layout: post
title:  "Understanding The Bitter Lesson"
date:   2020-03-13 
categories: AI, science
---
{% newthought 'This post' %} articulates my position on [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html), a conclusion that Rich Sutton draws in his retrospective critique of Artifical Intelligence (AI) research. 
<!--more--> 

## The Bitter Lesson in Summary
Sutton's bitter lesson teaches us that long-term progress in AI has predominately been the result of general methods that were endowed with more computational resources. The message is difficult for AI researchers to swallow, because many like to think their research informs general knowledge about *the mind*{% sidenote "sn-mind" "I refer to *the mind* as the set of mental processes that are coupled to the physical world. The term is intended to be a pedagogical device, not an invitation to explain cognition as a separate entity inside ourselves doing all the deciding and experiencing for us." %}: how it works and how it interacts with the world. Sutton provides a few notable examples showing that the opposite has been true. Encoding how we believe the mind works ultimately subtracts from a method's generality and hence its long-term impact. His examples illustrate that such approaches have historically been overshadowed by more general methods whose performance improves with Moore's Law. 

I think it's easy to be mistake Sutton's words for his ideas. The article can give the impression that any and all inductive biases should be discouraged, and that AI researchers should spend their energy on more 'brute force' methods. In a sense this is true, but the message I find behind his words is less absolute. Sutton is saying that we need to be mindful of how far inductive biases will get us. His advice is based on particular ideas of what it means to be general and how the mind interacts with the world. These are easy to miss or overlook on a first read. I'll spend the rest of the post unpacking these ideas and describing other ways of viewing these concepts. My goal is not to champion any one particular view, but rather to show what each view has to offer the science of AI. 

### Generalization
Most AI researchers would agree that we should be developing general methods that can solve more problems. However, agreeing on what it means for an algorithm to generalize is a subject of some debate. Sutton describes *generalization* as the propensity for an algorithm to scale with more computation: 

> One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. ---  [The Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)

This is a valid way to think about generalization, because systems that are given more computational resources will be able to solve more problems. I find his idea akin to computational complexity; if algorithm {% m %}A{% em %} scales better than {% m %}B{% em %}  in terms of some resource, then we would conclude {% m %}A{% em %} can solve more problems and is the more general algorithm.

Another way to think about generalization is in terms of robustness, or the ability to cope with surprise. We can say that 
>Generalization is the ability to use past experience to perform successfully in new and different situations.

The rationale here is that proficient adaptation enables more problems to be solved. This view emphasizes an important aspect of reality: the experience stream. In reality, what you see is all there is{% sidenote "sn-thinking_fast_and_slow" "An idea from psychology. See Daniel Kahneman's book, *Thinking Fast and Slow*" %}, and what you will see is all that matters. An increased propensity to scale in other situations matters less. Viewing generalization as robustness suggests that what is already known and how it is represented are important to problem solving. We can see some evidence for this from evolution {%  marginfigure  'mf-tree_of_life'  'assets/img/2020-bitter_lesson/tree-of-life.png'  'Leonard Eisenberg, *Tree of Life.* Humans seem to be more general problem solvers than other species, despite evolving for much less time. Perhaps we represent knowledge in a way that makes us more robust to the surprises of life.'  %}. What distinguishes humans from the rest of the natural world is not how long we took to evolve, but *how* we evolved from our unique history of experience. 

Both views of generality offer a way to solve more problems. Sutton's view is based on the premise that good complexity implies good generalization. His idea is historically backed and divorced from the biases humans try to build into their systems. The alternative view argues, however, that thinking entirely in terms of complexity is too removed from the full reality of how agents interact with the world.  In nature, having a larger brain, a higher neuronal density, or more time to evolve (e.g. more resources) doesn't always translate to an improved existence. What matters is the ability to cope with the surprises life presents. Resources can certainly help with this, but only when the right concepts, representations, and *inductive biases* are already in place.
  
### Inductive Biases
The Bitter Lesson cautions against making systems that work the way we think our own minds work. This is Sutton taking a stance on inductive bias. One thing he's saying is that inductive biases are not free{% sidenote "sn-inductive_bias" "This message is echoed in a recent paper  by his colleagues at DeepMind: On Inductive Biases in Deep Reinforcement Learning [[pdf]](https://arxiv.org/abs/1907.02908)" %}. From his perspective, inductive biases subtract from a method's ability to scale with more computation. He brings up several examples where the induction of domain-specific knowledge caused research progress to plateau. 

It should be clear that inductive bias is inherently linked to the notion of generality which one chooses to subscribe. Induction of prior knowledge can support better adaptation. And this can be positive if one views generality as an emerging property from robustness to surprise. Consider the application of convolutional neural networks to streams of imagery. Convolutions allow systems to adapt more effectively when their experience streams primarily consist of spatial data. Even Sutton agrees that computer vision methods perform much better using these principles: "Modern deep-learning neural networks use only the notions of convolution and certain kinds of invariances, and perform much better." It's undeniable that in some cases, fewer biases can lead to algorithms that work on a larger class of problems. However, this observation just reinforces the robustness view of generality, which proclaims that researchers should be looking the *right inductive biases.* How to find the right inductive bias is an open question and a topic of further debate. Regardless of which side one finds themself on this issue, both views underscore the importance of understanding the role of inductive biases in the context of generality. 

### Scientific Progress in Artifical Intelligence
In some ways The Bitter Lesson is a call for AI scientists to set their sights on a more distant horizon and consider the far term impact of their research. Given that computational resources are becoming more abundant all the time, this seems like sage advice. AI systems shouldn't be designed as if their available computation will remain constant. 

For Sutton, scientific progress seems like a straight path that gets broader as time goes on and more computation becomes available. There are sometimes places where we can make progress by going off path to circumvent difficult terrain, but for the most part these short deviations only create a tendency to get lost and to lose sight of the final objective. 

In the few years I've spent researching AI, I've observed that progress behaves more like a pendulum, whose length represents progress, and whose motion points to the dominant approach of advancement. The AI pendulum in this discussion oscillates between searching for good inductive biases and scaling our ideas with more computation. If we're doing things right as a community, then its average undulations will contract toward the center, and its swings will quicken as progress in one area informs work in the other. Viewing this process differently shows that the progress will trajectory spirals inward toward a breakthrough in our understanding general intelligence and what it means to replicate that.   

~John 
